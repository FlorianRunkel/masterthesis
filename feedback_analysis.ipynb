{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d33adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/')\n",
    "\n",
    "from backend.ml_pipe.data.database.mongodb import MongoDb\n",
    "\n",
    "# Matplotlib Einstellungen für deutsche Beschriftungen\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393cea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': '68c8349bcd07e1b6a3665a13', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 4, 3, 4, 5, 5, 5, 4, 5, 3, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:45:31.111053Z'}, {'_id': '68c834ea4c8877fd37754728', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 5, 4, 4, 3, 5, 5, 4, 4, 3, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:46:50.816364Z'}, {'_id': '68c8363e4c8877fd3775472a', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 5, 4, 4, 5, 5, 4, 3, 5, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:52:29.992306Z'}, {'_id': '68c8373dcd07e1b6a3665a15', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 5, 5, 5, 4, 5, 5, 4, 2, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:56:45.394317Z'}, {'_id': '68c837fb4c8877fd3775472c', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 4, 3, 4, 3, 5, 5, 4, 2, 2, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:59:55.437946Z'}, {'_id': '68c839ed4c8877fd3775472e', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [5, 4, 4, 3, 4, 5, 5, 5, 5, 5, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:08:13.571931Z'}, {'_id': '68c83a87cd07e1b6a3665a17', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 5, 4, 3, 4, 5, 5, 4, 4, 1, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:10:47.406243Z'}, {'_id': '68c83b42cd07e1b6a3665a19', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:13:54.289092Z'}, {'_id': '68c844f44c8877fd37754731', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 3, 4, 5, 5, 5, 3, 4, 3, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:55:16.299660Z'}, {'_id': '68c8451f4c8877fd37754733', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 4, 4, 5, 5, 4, 4, 3, 4, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:55:58.908833Z'}, {'_id': '68c874a7cd07e1b6a3665a1f', 'uid': 'UID002', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 4, 4, 2, 1, 4, 5, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T20:18:47.527981Z'}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mongo_client = MongoDb()\n",
    "\n",
    "    result = mongo_client.get_all('feedback')\n",
    "    feedback_data = result.get('data', [])\n",
    "\n",
    "    print(feedback_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MongoDB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6157f4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'statusCode': 200, 'data': [{'_id': '68c8349bcd07e1b6a3665a13', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 4, 3, 4, 5, 5, 5, 4, 5, 3, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:45:31.111053Z'}, {'_id': '68c834ea4c8877fd37754728', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 5, 4, 4, 3, 5, 5, 4, 4, 3, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:46:50.816364Z'}, {'_id': '68c8363e4c8877fd3775472a', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 5, 4, 4, 5, 5, 4, 3, 5, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:52:29.992306Z'}, {'_id': '68c8373dcd07e1b6a3665a15', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 5, 5, 5, 4, 5, 5, 4, 2, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:56:45.394317Z'}, {'_id': '68c837fb4c8877fd3775472c', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 4, 3, 4, 3, 5, 5, 4, 2, 2, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T15:59:55.437946Z'}, {'_id': '68c839ed4c8877fd3775472e', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [5, 4, 4, 3, 4, 5, 5, 5, 5, 5, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:08:13.571931Z'}, {'_id': '68c83a87cd07e1b6a3665a17', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 5, 4, 3, 4, 5, 5, 4, 4, 1, 5], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:10:47.406243Z'}, {'_id': '68c83b42cd07e1b6a3665a19', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:13:54.289092Z'}, {'_id': '68c844f44c8877fd37754731', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 3, 4, 5, 5, 5, 3, 4, 3, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:55:16.299660Z'}, {'_id': '68c8451f4c8877fd37754733', 'uid': 'UID001', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 5, 4, 4, 5, 5, 4, 4, 3, 4, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T16:55:58.908833Z'}, {'_id': '68c874a7cd07e1b6a3665a1f', 'uid': 'UID002', 'freeText': '', 'prognoseBewertung': [{'modell': '', 'prognose': '', 'echt': '', 'bemerkung': ''}], 'bewertungsskala': [4, 4, 4, 2, 1, 4, 5, 4], 'explanationFeedback': {}, 'timestamp': '2025-09-15T20:18:47.527981Z'}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYMONGO_DISABLE_IPV6\"] = \"1\"     # IPv6 erstmal ausschalten\n",
    "os.environ[\"DNSPYTHON_IPV6\"] = \"false\"\n",
    "\n",
    "import dns.resolver\n",
    "r = dns.resolver.Resolver(configure=True)\n",
    "r.nameservers = [\"1.1.1.1\", \"8.8.8.8\"]       # Router-DNS umgehen\n",
    "r.timeout = 2.0\n",
    "r.lifetime = 3.0\n",
    "dns.resolver.default_resolver = r            # global setzen\n",
    "\n",
    "# jetzt erst Deinen Code:\n",
    "mongo_client = MongoDb()\n",
    "result = mongo_client.get_all(\"feedback\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be81858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATENVERARBEITUNG\n",
      "==============================\n",
      "11 Feedback-Einträge geladen\n",
      "\n",
      "Bewertungsskala-Längen:\n",
      "rating_length\n",
      "8      1\n",
      "11    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "FEEDBACK-DATA OVERVIEW\n",
      "==================================================\n",
      "Total number of feedback: 11\n",
      "Control Group (8 Fragen): 1 Teilnehmer\n",
      "Experimental Group (11 Fragen): 10 Teilnehmer\n",
      "Unknown Group: 0 Teilnehmer\n",
      "\n",
      "ERSTE 3 ZEILEN:\n",
      "      uid         group  rating_length                        timestamp\n",
      "0  UID001  Experimental             11 2025-09-15 15:45:31.111053+00:00\n",
      "1  UID001  Experimental             11 2025-09-15 15:46:50.816364+00:00\n",
      "2  UID001  Experimental             11 2025-09-15 15:52:29.992306+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDATENVERARBEITUNG\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if result and 'data' in result:\n",
    "    feedback_data = result['data']\n",
    "    print(f\"{len(feedback_data)} Feedback-Einträge geladen\")\n",
    "else:\n",
    "    print(\"Keine Daten verfügbar\")\n",
    "    feedback_data = []\n",
    "\n",
    "if len(feedback_data) > 0:\n",
    "    df = pd.DataFrame(feedback_data)\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    df['rating_length'] = df['bewertungsskala'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    print(f\"\\nBewertungsskala-Längen:\")\n",
    "    print(df['rating_length'].value_counts().sort_index())\n",
    "\n",
    "    df['group'] = df['rating_length'].apply(lambda x: 'Experimental' if x == 11 else 'Control' if x == 8 else 'Unknown')\n",
    "\n",
    "    for i in range(11):\n",
    "        df[f'rating_{i+1}'] = df.apply(lambda row: \n",
    "            row['bewertungsskala'][i] if isinstance(row['bewertungsskala'], list) and len(row['bewertungsskala']) > i else np.nan, axis=1)\n",
    "\n",
    "    control_df = df[df['group'] == 'Control'].copy()\n",
    "    experimental_df = df[df['group'] == 'Experimental'].copy()\n",
    "\n",
    "    # Grundlegende Statistiken\n",
    "    print(\"\\nFEEDBACK-DATA OVERVIEW\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total number of feedback: {len(df)}\")  \n",
    "    print(f\"Control Group (8 Fragen): {len(control_df)} Teilnehmer\")\n",
    "    print(f\"Experimental Group (11 Fragen): {len(experimental_df)} Teilnehmer\")\n",
    "    print(f\"Unknown Group: {len(df[df['group'] == 'Unknown'])} Teilnehmer\")\n",
    "\n",
    "    # Zeige erste paar Zeilen\n",
    "    print(f\"\\nERSTE 3 ZEILEN:\")\n",
    "    print(df[['uid', 'group', 'rating_length', 'timestamp']].head(3).to_string())\n",
    "else:\n",
    "    print(\"Keine Daten zum Verarbeiten verfügbar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18292073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STATISTISCHE VERGLEICHE ZWISCHEN GRUPPEN:\n",
      "--------------------------------------------------\n",
      "\n",
      "RELEVANCE & REALISM:\n",
      "-------------------\n",
      "  Control Group:    M = 4.00, n = 2\n",
      "  Experimental Group: M = 4.15, n = 20\n",
      "  Mann-Whitney-U:   U = 17.000, p = 0.755\n",
      "  Cohen's d:        d = -0.212\n",
      "  → NICHT SIGNIFIKANT: Keine Unterschiede (p ≥ 0.05)\n",
      "\n",
      "CONFIDENCE IN PREDICTIONS:\n",
      "-------------------------\n",
      "  Control Group:    M = 3.00, n = 2\n",
      "  Experimental Group: M = 4.05, n = 20\n",
      "  Mann-Whitney-U:   U = 9.500, p = 0.208\n",
      "  Cohen's d:        d = -1.523\n",
      "  → NICHT SIGNIFIKANT: Keine Unterschiede (p ≥ 0.05)\n",
      "\n",
      "USABILITY FOR RECRUITING:\n",
      "------------------------\n",
      "  Control Group:    M = 2.50, n = 2\n",
      "  Experimental Group: M = 4.75, n = 20\n",
      "  Mann-Whitney-U:   U = 2.500, p = 0.013\n",
      "  Cohen's d:        d = -3.623\n",
      "  → SIGNIFIKANT: Control Group ist niedriger (p < 0.05)\n",
      "\n",
      "PERCEIVED VALUE & INTENTION TO USE:\n",
      "----------------------------------\n",
      "  Control Group:    M = 4.50, n = 2\n",
      "  Experimental Group: M = 3.80, n = 20\n",
      "  Mann-Whitney-U:   U = 26.500, p = 0.474\n",
      "  Cohen's d:        d = 0.613\n",
      "  → NICHT SIGNIFIKANT: Keine Unterschiede (p ≥ 0.05)\n",
      "\n",
      "ZUSAMMENFASSUNG ALLER VERGLEICHE:\n",
      "========================================\n",
      "Signifikante Unterschiede: 1/4 Kategorien\n",
      "\n",
      "Signifikante Kategorien:\n",
      "  • Usability for Recruiting: Control Group ist niedriger (p = 0.013, d = -3.623, groß)\n"
     ]
    }
   ],
   "source": [
    "# Control Group Questions (8 Fragen)\n",
    "control_group_questions = [\n",
    "    'The system\\'s predictions about candidate job-switching readiness seemed realistic.',\n",
    "    'The predictions were relevant for prioritizing candidates in Active Sourcing.',\n",
    "    'I trusted the system\\'s predictions when deciding which candidates to approach.',\n",
    "    'The recommendations gave me enough confidence to base sourcing decisions on them.',\n",
    "    'The system\\'s predictions were easy to interpret without further explanation.',\n",
    "    'The predictions helped me to structure the candidate selection process more efficiently.',\n",
    "    'I can imagine using such a prediction system in my daily recruiting activities.',\n",
    "    'The system would help me to improve the effectiveness of my sourcing decisions.'\n",
    "]\n",
    "\n",
    "# Experimental Group Questions (11 Fragen)\n",
    "experimental_group_questions = [\n",
    "    'The explanations made it clear why a candidate was predicted as more or less likely to switch jobs.',\n",
    "    'The explanations increased my understanding of how the system generated its predictions.',\n",
    "    'The explanations were concrete and applicable to my sourcing decisions.',\n",
    "    'The explanations strengthened my confidence in the reliability of the predictions.',\n",
    "    'The presence of explanations made me more willing to act on the system\\'s recommendations.',\n",
    "    'The combination of predictions and explanations was straightforward and clear to understand.',\n",
    "    'The explanations improved my ability to identify which candidates should be prioritized in Active Sourcing.',\n",
    "    'The explanations supported me in combining the system\\'s predictions with my own recruiting expertise.',\n",
    "    'The system complemented my judgment rather than replacing it.',\n",
    "    'I could imagine integrating such a system with explanations into my daily recruiting workflow.',\n",
    "    'The explanations provided added value compared to predictions alone.'\n",
    "]\n",
    "\n",
    "# Kategorien-Mapping für vergleichbare Fragen\n",
    "comparable_questions = {\n",
    "    'Relevance & Realism': {\n",
    "        'control': [1, 2],  # Q1, Q2\n",
    "        'experimental': [1, 2]  # Q1, Q2\n",
    "    },\n",
    "    'Confidence in Predictions': {\n",
    "        'control': [3, 4],  # Q3, Q4\n",
    "        'experimental': [4, 5]  # Q4, Q5\n",
    "    },\n",
    "    'Usability for Recruiting': {\n",
    "        'control': [5, 6],  # Q5, Q6\n",
    "        'experimental': [6, 7]  # Q6, Q7\n",
    "    },\n",
    "    'Perceived Value & Intention to Use': {\n",
    "        'control': [7, 8],  # Q7, Q8\n",
    "        'experimental': [10, 11]  # Q10, Q11\n",
    "    }\n",
    "}\n",
    "\n",
    "# Statistische Tests für alle vergleichbaren Kategorien\n",
    "print(\"\\nSTATISTISCHE VERGLEICHE ZWISCHEN GRUPPEN:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for category, question_indices in comparable_questions.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(\"-\" * len(category))\n",
    "    \n",
    "    # Control Group Daten sammeln\n",
    "    control_ratings = []\n",
    "    for idx in question_indices['control']:\n",
    "        col = f'rating_{idx}'\n",
    "        if col in control_df.columns:\n",
    "            ratings = control_df[col].dropna()\n",
    "            control_ratings.extend(ratings.tolist())\n",
    "    \n",
    "    # Experimental Group Daten sammeln\n",
    "    experimental_ratings = []\n",
    "    for idx in question_indices['experimental']:\n",
    "        col = f'rating_{idx}'\n",
    "        if col in experimental_df.columns:\n",
    "            ratings = experimental_df[col].dropna()\n",
    "            experimental_ratings.extend(ratings.tolist())\n",
    "    \n",
    "    if len(control_ratings) > 0 and len(experimental_ratings) > 0:\n",
    "        # Mann-Whitney-U Test\n",
    "        stat, p_value = mannwhitneyu(control_ratings, experimental_ratings, alternative='two-sided')\n",
    "        \n",
    "        # Effektstärke (Cohen's d)\n",
    "        n1, n2 = len(control_ratings), len(experimental_ratings)\n",
    "        pooled_std = np.sqrt(((n1-1)*np.var(control_ratings) + (n2-1)*np.var(experimental_ratings)) / (n1+n2-2))\n",
    "        cohens_d = (np.mean(control_ratings) - np.mean(experimental_ratings)) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        # Ergebnisse speichern\n",
    "        result = {\n",
    "            'category': category,\n",
    "            'control_mean': np.mean(control_ratings),\n",
    "            'experimental_mean': np.mean(experimental_ratings),\n",
    "            'control_n': len(control_ratings),\n",
    "            'experimental_n': len(experimental_ratings),\n",
    "            'u_statistic': stat,\n",
    "            'p_value': p_value,\n",
    "            'cohens_d': cohens_d,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "        comparison_results.append(result)\n",
    "        \n",
    "        # Ausgabe\n",
    "        print(f\"  Control Group:    M = {result['control_mean']:.2f}, n = {result['control_n']}\")\n",
    "        print(f\"  Experimental Group: M = {result['experimental_mean']:.2f}, n = {result['experimental_n']}\")\n",
    "        print(f\"  Mann-Whitney-U:   U = {stat:.3f}, p = {p_value:.3f}\")\n",
    "        print(f\"  Cohen's d:        d = {cohens_d:.3f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            direction = \"höher\" if result['control_mean'] > result['experimental_mean'] else \"niedriger\"\n",
    "            print(f\"  → SIGNIFIKANT: Control Group ist {direction} (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  → NICHT SIGNIFIKANT: Keine Unterschiede (p ≥ 0.05)\")\n",
    "    else:\n",
    "        print(f\"  → Nicht genügend Daten für Vergleich verfügbar\")\n",
    "\n",
    "# Zusammenfassung aller Vergleiche\n",
    "print(f\"\\nZUSAMMENFASSUNG ALLER VERGLEICHE:\")\n",
    "print(\"=\" * 40)\n",
    "significant_count = sum(1 for r in comparison_results if r['significant'])\n",
    "print(f\"Signifikante Unterschiede: {significant_count}/{len(comparison_results)} Kategorien\")\n",
    "\n",
    "if significant_count > 0:\n",
    "    print(f\"\\nSignifikante Kategorien:\")\n",
    "    for result in comparison_results:\n",
    "        if result['significant']:\n",
    "            direction = \"höher\" if result['control_mean'] > result['experimental_mean'] else \"niedriger\"\n",
    "            effect_size = \"groß\" if abs(result['cohens_d']) > 0.8 else \"mittel\" if abs(result['cohens_d']) > 0.5 else \"klein\"\n",
    "            print(f\"  • {result['category']}: Control Group ist {direction} (p = {result['p_value']:.3f}, d = {result['cohens_d']:.3f}, {effect_size})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfe47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STATISTISCHE TESTS\n",
      "==============================\n",
      "\n",
      "1. NORMALITÄTSTESTS (Shapiro-Wilk) - GRUPPENSPEZIFISCH\n",
      "=================================================================\n",
      "\n",
      "CONTROL GROUP NORMALITÄTSTESTS:\n",
      "----------------------------------------\n",
      "Keine Control Group Daten verfügbar\n",
      "\n",
      "EXPERIMENTAL GROUP NORMALITÄTSTESTS:\n",
      "---------------------------------------------\n",
      "Frage  N  Shapiro-W  p-Wert Normalverteilt\n",
      "   Q1 10     0.8022  0.0154           Nein\n",
      "   Q2 10     0.6405  0.0002           Nein\n",
      "   Q3 10     0.8325  0.0359           Nein\n",
      "   Q4 10     0.7516  0.0037           Nein\n",
      "   Q5 10     0.8197  0.0251           Nein\n",
      "   Q6 10     0.5093  0.0000           Nein\n",
      "   Q7 10     0.5316  0.0000           Nein\n",
      "   Q8 10     0.8148  0.0219           Nein\n",
      "   Q9 10     0.9108  0.2869             Ja\n",
      "  Q10 10     0.9240  0.3915             Ja\n",
      "  Q11 10     0.6553  0.0003           Nein\n",
      "\n",
      "GESAMTINTERPRETATION: 9/11 Fragen sind nicht normalverteilt\n",
      "→ Nicht-parametrische Tests (wie der Kruskal-Wallis) sind angemessen\n"
     ]
    }
   ],
   "source": [
    "# STATISTISCHE TESTS: NORMALITÄT UND GRUPPENVERGLEICHE\n",
    "print(\"\\nSTATISTISCHE TESTS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, kruskal, wilcoxon\n",
    "\n",
    "# 1. NORMALITÄTSTESTS (Shapiro-Wilk) - GRUPPENSPEZIFISCH\n",
    "print(\"\\n1. NORMALITÄTSTESTS (Shapiro-Wilk) - GRUPPENSPEZIFISCH\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Control Group Normalitätstests\n",
    "print(\"\\nCONTROL GROUP NORMALITÄTSTESTS:\")\n",
    "print(\"-\" * 40)\n",
    "control_normality_results = []\n",
    "for i, question in enumerate(control_group_questions):\n",
    "    col = f'rating_{i+1}'\n",
    "    if col in control_df.columns:\n",
    "        ratings = control_df[col].dropna()\n",
    "        if len(ratings) >= 3:  # Mindestens 3 Werte für Shapiro-Wilk\n",
    "            stat, p_value = shapiro(ratings)\n",
    "            control_normality_results.append({\n",
    "                'Frage': f\"Q{i+1}\",\n",
    "                'N': len(ratings),\n",
    "                'Shapiro-W': round(stat, 4),\n",
    "                'p-Wert': round(p_value, 4),\n",
    "                'Normalverteilt': 'Ja' if p_value > 0.05 else 'Nein'\n",
    "            })\n",
    "\n",
    "if control_normality_results:\n",
    "    control_normality_df = pd.DataFrame(control_normality_results)\n",
    "    print(control_normality_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Keine Control Group Daten verfügbar\")\n",
    "\n",
    "# Experimental Group Normalitätstests\n",
    "print(\"\\nEXPERIMENTAL GROUP NORMALITÄTSTESTS:\")\n",
    "print(\"-\" * 45)\n",
    "experimental_normality_results = []\n",
    "for i, question in enumerate(experimental_group_questions):\n",
    "    col = f'rating_{i+1}'\n",
    "    if col in experimental_df.columns:\n",
    "        ratings = experimental_df[col].dropna()\n",
    "        if len(ratings) >= 3:  # Mindestens 3 Werte für Shapiro-Wilk\n",
    "            stat, p_value = shapiro(ratings)\n",
    "            experimental_normality_results.append({\n",
    "                'Frage': f\"Q{i+1}\",\n",
    "                'N': len(ratings),\n",
    "                'Shapiro-W': round(stat, 4),\n",
    "                'p-Wert': round(p_value, 4),\n",
    "                'Normalverteilt': 'Ja' if p_value > 0.05 else 'Nein'\n",
    "            })\n",
    "\n",
    "if experimental_normality_results:\n",
    "    experimental_normality_df = pd.DataFrame(experimental_normality_results)\n",
    "    print(experimental_normality_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Keine Experimental Group Daten verfügbar\")\n",
    "\n",
    "# Gesamtinterpretation\n",
    "all_normality_results = control_normality_results + experimental_normality_results\n",
    "non_normal_count = sum(1 for result in all_normality_results if result['Normalverteilt'] == 'Nein')\n",
    "print(f\"\\nGESAMTINTERPRETATION: {non_normal_count}/{len(all_normality_results)} Fragen sind nicht normalverteilt\")\n",
    "print(\"→ Nicht-parametrische Tests (wie der Kruskal-Wallis) sind angemessen\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c323",
   "metadata": {},
   "source": [
    "## Kruskal Walis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a86eb64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONTROL GROUP KRUSKAL-WALLIS-TEST\n",
      "===================================\n",
      "Testet, ob es signifikante Unterschiede zwischen den Fragen gibt\n",
      "H-Statistik: 7.0000\n",
      "p-Wert: 0.4289\n",
      "✗ NICHT SIGNIFIKANT: Keine Unterschiede (p ≥ 0.05)\n",
      "Effektstärke (ε²): nan\n",
      "\n",
      "EXPERIMENTAL GROUP KRUSKAL-WALLIS-TEST\n",
      "========================================\n",
      "Testet, ob es signifikante Unterschiede zwischen den Fragen gibt\n",
      "H-Statistik: 34.2245\n",
      "p-Wert: 0.0002\n",
      "✓ SIGNIFIKANT: Unterschiede zwischen den Fragen (p < 0.05)\n",
      "Effektstärke (ε²): 0.2447\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "import numpy as np\n",
    "\n",
    "def run_kw_test(df, question_list, label, alpha=0.05):\n",
    "    print(f\"\\n{label.upper()} KRUSKAL-WALLIS-TEST\")\n",
    "    print(\"=\" * (len(label)+22))\n",
    "    print(\"Testet, ob es signifikante Unterschiede zwischen den Fragen gibt\")\n",
    "\n",
    "    groups = []\n",
    "    names = []\n",
    "\n",
    "    for i, q in enumerate(question_list):\n",
    "        col = f\"rating_{i+1}\"\n",
    "        if col in df.columns:\n",
    "            vals = df[col].dropna().values\n",
    "            if len(vals) > 0:\n",
    "                groups.append(vals)\n",
    "                names.append(f\"Q{i+1}\")\n",
    "\n",
    "    if len(groups) < 2:\n",
    "        print(\"Nicht genügend Daten (mind. 2 Fragen mit Werten).\")\n",
    "        return\n",
    "\n",
    "    # Kruskal-Wallis-Test\n",
    "    h_stat, p_value = kruskal(*groups)\n",
    "\n",
    "    print(f\"H-Statistik: {h_stat:.4f}\")\n",
    "    print(f\"p-Wert: {p_value:.4f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"✓ SIGNIFIKANT: Unterschiede zwischen den Fragen (p < {alpha})\")\n",
    "    else:\n",
    "        print(f\"✗ NICHT SIGNIFIKANT: Keine Unterschiede (p ≥ {alpha})\")\n",
    "\n",
    "    # Effektstärke ε² (für KW)\n",
    "    n_total = sum(len(g) for g in groups)\n",
    "    k = len(groups)\n",
    "    eps2 = (h_stat - (k - 1)) / (n_total - k) if (n_total - k) > 0 else np.nan\n",
    "    print(f\"Effektstärke (ε²): {eps2:.4f}\")\n",
    "\n",
    "\n",
    "# Beispiel-Aufrufe:\n",
    "run_kw_test(control_df, control_group_questions, \"Control Group\")\n",
    "run_kw_test(experimental_df, experimental_group_questions, \"Experimental Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b5316a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POST-HOC DUNN-TEST (Control Group) – signifikante Paare (Holm-korrigiert, alpha=0.05)\n",
      "========================================================================\n",
      "Keine signifikanten Unterschiede.\n",
      "\n",
      "Komplette p-Wert-Matrix (Holm-korrigiert):\n",
      "     Q1   Q2   Q3   Q4      Q5   Q6      Q7   Q8\n",
      "Q1  1.0  1.0  1.0  1.0  1.0000  1.0  1.0000  1.0\n",
      "Q2  1.0  1.0  1.0  1.0  1.0000  1.0  1.0000  1.0\n",
      "Q3  1.0  1.0  1.0  1.0  1.0000  1.0  1.0000  1.0\n",
      "Q4  1.0  1.0  1.0  1.0  1.0000  1.0  1.0000  1.0\n",
      "Q5  1.0  1.0  1.0  1.0  1.0000  1.0  0.5771  1.0\n",
      "Q6  1.0  1.0  1.0  1.0  1.0000  1.0  1.0000  1.0\n",
      "Q7  1.0  1.0  1.0  1.0  0.5771  1.0  1.0000  1.0\n",
      "Q8  1.0  1.0  1.0  1.0  1.0000  1.0  1.0000  1.0\n",
      "\n",
      "POST-HOC DUNN-TEST (Experimental Group) – signifikante Paare (Holm-korrigiert, alpha=0.05)\n",
      "=============================================================================\n",
      "- Q10 [Perceived Value & Intention to Use]  vs.  Q6 [Usability for Recruiting]  | p_adj = 0.0054\n",
      "    Q10: I could imagine integrating such a system with explanations into my daily recruiting workflow.\n",
      "    Q6: The combination of predictions and explanations was straightforward and clear to understand.\n",
      "\n",
      "- Q10 [Perceived Value & Intention to Use]  vs.  Q7 [Usability for Recruiting]  | p_adj = 0.0140\n",
      "    Q10: I could imagine integrating such a system with explanations into my daily recruiting workflow.\n",
      "    Q7: The explanations improved my ability to identify which candidates should be prioritized in Active Sourcing.\n",
      "\n",
      "\n",
      "Komplette p-Wert-Matrix (Holm-korrigiert):\n",
      "         Q1     Q10     Q11      Q2      Q3      Q4   Q5      Q6      Q7  \\\n",
      "Q1   1.0000  1.0000  0.9901  0.4666  1.0000  1.0000  1.0  0.0716  0.1557   \n",
      "Q10  1.0000  1.0000  0.1602  0.0592  1.0000  1.0000  1.0  0.0054  0.0140   \n",
      "Q11  0.9901  0.1602  1.0000  1.0000  1.0000  1.0000  1.0  1.0000  1.0000   \n",
      "Q2   0.4666  0.0592  1.0000  1.0000  1.0000  1.0000  1.0  1.0000  1.0000   \n",
      "Q3   1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0  0.3835  0.7148   \n",
      "Q4   1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0  0.3134  0.5949   \n",
      "Q5   1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0  1.0000  1.0000   \n",
      "Q6   0.0716  0.0054  1.0000  1.0000  0.3835  0.3134  1.0  1.0000  1.0000   \n",
      "Q7   0.1557  0.0140  1.0000  1.0000  0.7148  0.5949  1.0  1.0000  1.0000   \n",
      "Q8   1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0  0.7148  1.0000   \n",
      "Q9   1.0000  1.0000  1.0000  0.8012  1.0000  1.0000  1.0  0.1513  0.3059   \n",
      "\n",
      "         Q8      Q9  \n",
      "Q1   1.0000  1.0000  \n",
      "Q10  1.0000  1.0000  \n",
      "Q11  1.0000  1.0000  \n",
      "Q2   1.0000  0.8012  \n",
      "Q3   1.0000  1.0000  \n",
      "Q4   1.0000  1.0000  \n",
      "Q5   1.0000  1.0000  \n",
      "Q6   0.7148  0.1513  \n",
      "Q7   1.0000  0.3059  \n",
      "Q8   1.0000  1.0000  \n",
      "Q9   1.0000  1.0000  \n"
     ]
    }
   ],
   "source": [
    "import scikit_posthocs as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def run_dunn_for_group(df, question_texts, group_label, comparable_questions, side_key, alpha=0.05):\n",
    "    data_vals, data_labels = [], []\n",
    "    for i, q in enumerate(question_texts):\n",
    "        col = f\"rating_{i+1}\"\n",
    "        if col in df.columns:\n",
    "            vals = df[col].dropna().values\n",
    "            if len(vals) > 0:\n",
    "                data_vals.extend(vals)\n",
    "                data_labels.extend([f\"Q{i+1}\"] * len(vals))\n",
    "\n",
    "    if len(set(data_labels)) < 2:\n",
    "        print(f\"\\n{group_label.upper()} – zu wenige Fragen mit Daten (mind. 2).\")\n",
    "        return\n",
    "\n",
    "    long_df = pd.DataFrame({\"rating\": data_vals, \"question\": data_labels})\n",
    "    posthoc = sp.posthoc_dunn(long_df, val_col='rating', group_col='question', p_adjust='holm')\n",
    "\n",
    "    q_to_text = {f\"Q{i+1}\": txt for i, txt in enumerate(question_texts)}\n",
    "    q_to_cat = {}\n",
    "    for cat, sides in comparable_questions.items():\n",
    "        for qnum in sides.get(side_key, []):\n",
    "            q_to_cat[f\"Q{qnum}\"] = cat\n",
    "\n",
    "    m = posthoc.copy()\n",
    "    m.index.name = \"q_i\"\n",
    "    m.columns.name = \"q_j\"\n",
    "    long_mat = m.reset_index().melt(id_vars=\"q_i\", var_name=\"q_j\", value_name=\"p_adj\")\n",
    "    long_mat = long_mat[long_mat[\"q_i\"] < long_mat[\"q_j\"]].dropna(subset=[\"p_adj\"])\n",
    "\n",
    "    sig = long_mat[long_mat[\"p_adj\"] < alpha].sort_values(\"p_adj\").reset_index(drop=True)\n",
    "    sig[\"q_i_text\"] = sig[\"q_i\"].map(q_to_text)\n",
    "    sig[\"q_j_text\"] = sig[\"q_j\"].map(q_to_text)\n",
    "    sig[\"cat_i\"] = sig[\"q_i\"].map(q_to_cat)\n",
    "    sig[\"cat_j\"] = sig[\"q_j\"].map(q_to_cat)\n",
    "\n",
    "    # 5) Ausgabe\n",
    "    print(f\"\\nPOST-HOC DUNN-TEST ({group_label}) – signifikante Paare (Holm-korrigiert, alpha={alpha})\")\n",
    "    print(\"=\" * (len(group_label) + 59))\n",
    "    if sig.empty:\n",
    "        print(\"Keine signifikanten Unterschiede.\")\n",
    "    else:\n",
    "        for _, row in sig.iterrows():\n",
    "            ci = f\"[{row['cat_i']}]\" if pd.notna(row['cat_i']) else \"\"\n",
    "            cj = f\"[{row['cat_j']}]\" if pd.notna(row['cat_j']) else \"\"\n",
    "            print(f\"- {row['q_i']} {ci}  vs.  {row['q_j']} {cj}  | p_adj = {row['p_adj']:.4f}\")\n",
    "            print(f\"    {row['q_i']}: {row['q_i_text']}\")\n",
    "            print(f\"    {row['q_j']}: {row['q_j_text']}\\n\")\n",
    "\n",
    "    print(\"\\nKomplette p-Wert-Matrix (Holm-korrigiert):\")\n",
    "    print(posthoc.round(4))\n",
    "\n",
    "run_dunn_for_group(\n",
    "    df=control_df,\n",
    "    question_texts=control_group_questions,\n",
    "    group_label=\"Control Group\",\n",
    "    comparable_questions=comparable_questions,\n",
    "    side_key=\"control\",\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "run_dunn_for_group(\n",
    "    df=experimental_df,\n",
    "    question_texts=experimental_group_questions,\n",
    "    group_label=\"Experimental Group\",\n",
    "    comparable_questions=comparable_questions,\n",
    "    side_key=\"experimental\",\n",
    "    alpha=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c2d64",
   "metadata": {},
   "source": [
    "## Wilcoxon-Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357c0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONTROL GROUP – keine auswertbaren Paare (min_pairs=2).\n",
      "\n",
      "WILCOXON SIGNED-RANK – PAARVERGLEICHE (Experimental Group) (Holm-korrigiert, alpha=0.05, min_pairs=2)\n",
      "===============================================================================\n",
      "Keine signifikanten Unterschiede.\n",
      "\n",
      "Komplette p-Wert-Matrix (roh):\n",
      "         Q1      Q2      Q3      Q4      Q5      Q6      Q7      Q8      Q9  \\\n",
      "Q1   1.0000  0.0209  0.4795  0.4795  0.0588  0.0094  0.0152  0.1797  1.0000   \n",
      "Q2   0.0209  1.0000  0.0196  0.0196  0.1573  0.3173  0.6547  0.0578  0.0488   \n",
      "Q3   0.4795  0.0196  1.0000  1.0000  0.3340  0.0304  0.0461  0.6547  0.6078   \n",
      "Q4   0.4795  0.0196  1.0000  1.0000  0.2568  0.0209  0.0335  0.7055  0.6078   \n",
      "Q5   0.0588  0.1573  0.3340  0.2568  1.0000  0.0836  0.1597  0.5271  0.1317   \n",
      "Q6   0.0094  0.3173  0.0304  0.0209  0.0836  1.0000  0.5637  0.0209  0.0158   \n",
      "Q7   0.0152  0.6547  0.0461  0.0335  0.1597  0.5637  1.0000  0.0196  0.0141   \n",
      "Q8   0.1797  0.0578  0.6547  0.7055  0.5271  0.0209  0.0196  1.0000  0.3173   \n",
      "Q9   1.0000  0.0488  0.6078  0.6078  0.1317  0.0158  0.0141  0.3173  1.0000   \n",
      "Q10  0.0836  0.0164  0.0836  0.1303  0.0462  0.0106  0.0256  0.0707  0.2299   \n",
      "Q11  0.0231  0.6547  0.0578  0.0836  0.3657  0.0833  0.3173  0.0588  0.0231   \n",
      "\n",
      "        Q10     Q11  \n",
      "Q1   0.0836  0.0231  \n",
      "Q2   0.0164  0.6547  \n",
      "Q3   0.0836  0.0578  \n",
      "Q4   0.1303  0.0836  \n",
      "Q5   0.0462  0.3657  \n",
      "Q6   0.0106  0.0833  \n",
      "Q7   0.0256  0.3173  \n",
      "Q8   0.0707  0.0588  \n",
      "Q9   0.2299  0.0231  \n",
      "Q10  1.0000  0.0158  \n",
      "Q11  0.0158  1.0000  \n",
      "\n",
      "Komplette p-Wert-Matrix (Holm-korrigiert):\n",
      "         Q1      Q2      Q3      Q4   Q5      Q6      Q7      Q8      Q9  \\\n",
      "Q1   1.0000  0.9423  1.0000  1.0000  1.0  0.5156  0.7897  1.0000  1.0000   \n",
      "Q2   0.9423  1.0000  0.9423  0.9423  1.0  1.0000  1.0000  1.0000  1.0000   \n",
      "Q3   1.0000  0.9423  1.0000  1.0000  1.0  1.0000  1.0000  1.0000  1.0000   \n",
      "Q4   1.0000  0.9423  1.0000  1.0000  1.0  0.9423  1.0000  1.0000  1.0000   \n",
      "Q5   1.0000  1.0000  1.0000  1.0000  1.0  1.0000  1.0000  1.0000  1.0000   \n",
      "Q6   0.5156  1.0000  1.0000  0.9423  1.0  1.0000  1.0000  0.9423  0.8040   \n",
      "Q7   0.7897  1.0000  1.0000  1.0000  1.0  1.0000  1.0000  0.9423  0.7451   \n",
      "Q8   1.0000  1.0000  1.0000  1.0000  1.0  0.9423  0.9423  1.0000  1.0000   \n",
      "Q9   1.0000  1.0000  1.0000  1.0000  1.0  0.8040  0.7451  1.0000  1.0000   \n",
      "Q10  1.0000  0.8055  1.0000  1.0000  1.0  0.5731  1.0000  1.0000  1.0000   \n",
      "Q11  0.9719  1.0000  1.0000  1.0000  1.0  1.0000  1.0000  1.0000  0.9719   \n",
      "\n",
      "        Q10     Q11  \n",
      "Q1   1.0000  0.9719  \n",
      "Q2   0.8055  1.0000  \n",
      "Q3   1.0000  1.0000  \n",
      "Q4   1.0000  1.0000  \n",
      "Q5   1.0000  1.0000  \n",
      "Q6   0.5731  1.0000  \n",
      "Q7   1.0000  1.0000  \n",
      "Q8   1.0000  1.0000  \n",
      "Q9   1.0000  0.9719  \n",
      "Q10  1.0000  0.8040  \n",
      "Q11  0.8040  1.0000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def holm_correction(pvals_dict):\n",
    "    items = sorted(pvals_dict.items(), key=lambda kv: kv[1]) \n",
    "    m = len(items)\n",
    "    adj = {}\n",
    "    running_max = 0.0\n",
    "    for rank, ((i, j), p) in enumerate(items, start=1):\n",
    "        padj = (m - rank + 1) * p\n",
    "        padj = min(1.0, padj)\n",
    "        running_max = max(running_max, padj)\n",
    "        adj[(i, j)] = running_max\n",
    "    return adj\n",
    "\n",
    "def pairwise_wilcoxon(df_wide, q_labels, min_pairs=5):\n",
    "    p_raw = {}\n",
    "    n_pairs = {}\n",
    "    for i, j in combinations(q_labels, 2):\n",
    "        s1 = df_wide[i]\n",
    "        s2 = df_wide[j]\n",
    "        mask = s1.notna() & s2.notna()\n",
    "        x = s1[mask].to_numpy()\n",
    "        y = s2[mask].to_numpy()\n",
    "        n = len(x)\n",
    "        n_pairs[(i, j)] = n\n",
    "        if n < min_pairs:\n",
    "            p_raw[(i, j)] = np.nan\n",
    "            continue\n",
    "        try:\n",
    "            stat, p = wilcoxon(x, y, zero_method=\"wilcox\", alternative=\"two-sided\", correction=False, method=\"auto\")\n",
    "        except ValueError:\n",
    "            p = 1.0\n",
    "        p_raw[(i, j)] = p\n",
    "    return p_raw, n_pairs\n",
    "\n",
    "def run_wilcoxon_for_group(df, question_texts, group_label, comparable_questions, side_key, alpha=0.05, min_pairs=5):\n",
    "\n",
    "    cols = {}\n",
    "    for i, _ in enumerate(question_texts):\n",
    "        col_raw = f\"rating_{i+1}\"\n",
    "        if col_raw in df.columns:\n",
    "            cols[f\"Q{i+1}\"] = df[col_raw]\n",
    "    if len(cols) < 2:\n",
    "        print(f\"\\n{group_label.upper()} – zu wenige Fragen (mind. 2 Spalten vorhanden).\")\n",
    "        return\n",
    "\n",
    "    wide = pd.DataFrame(cols)\n",
    "\n",
    "    q_to_text = {f\"Q{i+1}\": txt for i, txt in enumerate(question_texts)}\n",
    "    q_to_cat = {}\n",
    "    for cat, sides in comparable_questions.items():\n",
    "        for qnum in sides.get(side_key, []):\n",
    "            q_to_cat[f\"Q{qnum}\"] = cat\n",
    "\n",
    "    q_labels = list(wide.columns)\n",
    "\n",
    "    p_raw, n_pairs = pairwise_wilcoxon(wide, q_labels, min_pairs=min_pairs)\n",
    "    p_raw_valid = {k: v for k, v in p_raw.items() if not np.isnan(v)}\n",
    "    if len(p_raw_valid) == 0:\n",
    "        print(f\"\\n{group_label.upper()} – keine auswertbaren Paare (min_pairs={min_pairs}).\")\n",
    "        return\n",
    "\n",
    "    p_adj = holm_correction(p_raw_valid)\n",
    "\n",
    "    all_labels = q_labels\n",
    "    mat_raw = pd.DataFrame(1.0, index=all_labels, columns=all_labels, dtype=float)\n",
    "    mat_adj = pd.DataFrame(1.0, index=all_labels, columns=all_labels, dtype=float)\n",
    "    for (i, j), p in p_raw.items():\n",
    "        mat_raw.loc[i, j] = mat_raw.loc[j, i] = p if not np.isnan(p) else np.nan\n",
    "    for (i, j), p in p_adj.items():\n",
    "        mat_adj.loc[i, j] = mat_adj.loc[j, i] = p\n",
    "\n",
    "    sig_rows = []\n",
    "    for (i, j), p in p_adj.items():\n",
    "        if p < alpha:\n",
    "            sig_rows.append({\n",
    "                \"q_i\": i, \"q_j\": j, \"p_adj\": p,\n",
    "                \"n_pairs\": n_pairs[(i, j)],\n",
    "                \"cat_i\": q_to_cat.get(i, np.nan),\n",
    "                \"cat_j\": q_to_cat.get(j, np.nan),\n",
    "                \"q_i_text\": q_to_text.get(i, i),\n",
    "                \"q_j_text\": q_to_text.get(j, j),\n",
    "            })\n",
    "    sig_df = pd.DataFrame(sig_rows).sort_values(\"p_adj\") if sig_rows else pd.DataFrame(columns=[\n",
    "        \"q_i\",\"q_j\",\"p_adj\",\"n_pairs\",\"cat_i\",\"cat_j\",\"q_i_text\",\"q_j_text\"\n",
    "    ])\n",
    "\n",
    "    # Ausgabe\n",
    "    print(f\"\\nWILCOXON SIGNED-RANK – PAARVERGLEICHE ({group_label}) (Holm-korrigiert, alpha={alpha}, min_pairs={min_pairs})\")\n",
    "    print(\"=\" * (len(group_label) + 61))\n",
    "    if sig_df.empty:\n",
    "        print(\"Keine signifikanten Unterschiede.\")\n",
    "    else:\n",
    "        for _, row in sig_df.iterrows():\n",
    "            ci = f\"[{row['cat_i']}]\" if pd.notna(row['cat_i']) else \"\"\n",
    "            cj = f\"[{row['cat_j']}]\" if pd.notna(row['cat_j']) else \"\"\n",
    "            print(f\"- {row['q_i']} {ci}  vs.  {row['q_j']} {cj}  | p_adj = {row['p_adj']:.4f} | n={int(row['n_pairs'])}\")\n",
    "            print(f\"    {row['q_i']}: {row['q_i_text']}\")\n",
    "            print(f\"    {row['q_j']}: {row['q_j_text']}\\n\")\n",
    "\n",
    "    print(\"\\nKomplette p-Wert-Matrix (roh):\")\n",
    "    print(mat_raw.round(4))\n",
    "    print(\"\\nKomplette p-Wert-Matrix (Holm-korrigiert):\")\n",
    "    print(mat_adj.round(4))\n",
    "\n",
    "run_wilcoxon_for_group(\n",
    "    df=control_df,\n",
    "    question_texts=control_group_questions,\n",
    "    group_label=\"Control Group\",\n",
    "    comparable_questions=comparable_questions,\n",
    "    side_key=\"control\",\n",
    "    alpha=0.05,\n",
    "    min_pairs=2\n",
    ")\n",
    "\n",
    "run_wilcoxon_for_group(\n",
    "    df=experimental_df,\n",
    "    question_texts=experimental_group_questions,\n",
    "    group_label=\"Experimental Group\",\n",
    "    comparable_questions=comparable_questions,\n",
    "    side_key=\"experimental\",\n",
    "    alpha=0.05,\n",
    "    min_pairs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "076a34c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONTROL GROUP – keine auswertbaren Kategorie-Paare (min_pairs=5).\n",
      "\n",
      "WILCOXON – KATEGORIEVERGLEICHE (Experimental Group) (agg=mean, Holm, alpha=0.05, min_pairs=5)\n",
      "===============================================================================\n",
      "Keine signifikanten Unterschiede zwischen Kategorien.\n",
      "\n",
      "Komplette p-Wert-Matrix (Holm-korrigiert):\n",
      "                                    Relevance & Realism  \\\n",
      "Relevance & Realism                              1.0000   \n",
      "Confidence in Predictions                        0.9844   \n",
      "Usability for Recruiting                         0.0981   \n",
      "Perceived Value & Intention to Use               0.3619   \n",
      "\n",
      "                                    Confidence in Predictions  \\\n",
      "Relevance & Realism                                    0.9844   \n",
      "Confidence in Predictions                              1.0000   \n",
      "Usability for Recruiting                               0.1392   \n",
      "Perceived Value & Intention to Use                     0.9844   \n",
      "\n",
      "                                    Usability for Recruiting  \\\n",
      "Relevance & Realism                                   0.0981   \n",
      "Confidence in Predictions                             0.1392   \n",
      "Usability for Recruiting                              1.0000   \n",
      "Perceived Value & Intention to Use                    0.0981   \n",
      "\n",
      "                                    Perceived Value & Intention to Use  \n",
      "Relevance & Realism                                             0.3619  \n",
      "Confidence in Predictions                                       0.9844  \n",
      "Usability for Recruiting                                        0.0981  \n",
      "Perceived Value & Intention to Use                              1.0000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Holm-Korrektur\n",
    "# -------------------------------\n",
    "def holm_correction(pvals_dict):\n",
    "    items = sorted(pvals_dict.items(), key=lambda kv: kv[1])  # aufsteigend nach p\n",
    "    m = len(items)\n",
    "    adj = {}\n",
    "    running_max = 0.0\n",
    "    for rank, (key, p) in enumerate(items, start=1):\n",
    "        padj = min(1.0, (m - rank + 1) * p)\n",
    "        running_max = max(running_max, padj)  # step-up\n",
    "        adj[key] = running_max\n",
    "    return adj\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Kategorie-Scores bauen\n",
    "# -------------------------------\n",
    "def build_category_scores(df, question_texts, comparable_questions, side_key, agg=\"mean\"):\n",
    "    \"\"\"\n",
    "    df: DataFrame mit Spalten rating_1 ... rating_k\n",
    "    question_texts: Liste der Fragetexte (Länge k)\n",
    "    comparable_questions: Dict Deiner Kategorien\n",
    "    side_key: \"control\" oder \"experimental\"\n",
    "    agg: \"mean\" oder \"median\"\n",
    "    return: DataFrame mit Spalten = Kategorienamen, Zeilen = Personen\n",
    "    \"\"\"\n",
    "    # Map Q-Label -> Spaltenname im DF\n",
    "    q_to_col = {f\"Q{i+1}\": f\"rating_{i+1}\" for i in range(len(question_texts))}\n",
    "\n",
    "    cat_frames = {}\n",
    "    for cat, sides in comparable_questions.items():\n",
    "        idxs = sides.get(side_key, [])\n",
    "        qs = [f\"Q{q}\" for q in idxs]\n",
    "        cols = [q_to_col[q] for q in qs if q_to_col[q] in df.columns]\n",
    "        if not cols:\n",
    "            continue\n",
    "        block = df[cols]\n",
    "        if agg == \"mean\":\n",
    "            cat_frames[cat] = block.mean(axis=1)\n",
    "        elif agg == \"median\":\n",
    "            cat_frames[cat] = block.median(axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"agg muss 'mean' oder 'median' sein.\")\n",
    "    if not cat_frames:\n",
    "        return pd.DataFrame()\n",
    "    return pd.DataFrame(cat_frames)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Paarweiser Wilcoxon auf Kategorien\n",
    "# -------------------------------\n",
    "def pairwise_wilcoxon_categories(cat_df, min_pairs=5):\n",
    "    \"\"\"\n",
    "    cat_df: DataFrame mit Kategorien als Spalten\n",
    "    return:\n",
    "      p_raw: {(c1,c2): p}\n",
    "      stats: {(c1,c2): {\"n\": n, \"W\": stat, \"r_rb\": r}}\n",
    "    \"\"\"\n",
    "    cats = list(cat_df.columns)\n",
    "    p_raw, stats = {}, {}\n",
    "    for c1, c2 in combinations(cats, 2):\n",
    "        x = cat_df[c1]\n",
    "        y = cat_df[c2]\n",
    "        mask = x.notna() & y.notna()\n",
    "        x_, y_ = x[mask].to_numpy(), y[mask].to_numpy()\n",
    "        n = len(x_)\n",
    "        if n < min_pairs:\n",
    "            continue\n",
    "        try:\n",
    "            # zweiseitig; W = Summe der positiven Ränge\n",
    "            W, p = wilcoxon(x_, y_, zero_method=\"wilcox\", alternative=\"two-sided\",\n",
    "                            correction=False, method=\"auto\")\n",
    "        except ValueError:\n",
    "            # z.B. alle Differenzen genau 0 -> kein Test möglich\n",
    "            continue\n",
    "        p_raw[(c1, c2)] = float(p)\n",
    "        # Rank-biserial correlation (für gepaarte Wilcoxon):\n",
    "        # r_rb = 1 - (2*W) / (n*(n+1))\n",
    "        r_rb = 1.0 - (2.0 * W) / (n * (n + 1))\n",
    "        stats[(c1, c2)] = {\"n\": n, \"W\": float(W), \"r_rb\": float(r_rb)}\n",
    "    return p_raw, stats\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Hauptfunktion für eine Gruppe\n",
    "# -------------------------------\n",
    "def run_wilcoxon_categories(df, question_texts, group_label,\n",
    "                            comparable_questions, side_key,\n",
    "                            alpha=0.05, min_pairs=5, agg=\"mean\"):\n",
    "    \"\"\"\n",
    "    Führt Wilcoxon-Tests zwischen Kategorie-Scores (gepairt) durch.\n",
    "    Druckt signifikante Paare + komplette p-Matrix (adj).\n",
    "    \"\"\"\n",
    "    cat_df = build_category_scores(df, question_texts, comparable_questions, side_key, agg=agg)\n",
    "    if cat_df.empty or cat_df.shape[1] < 2:\n",
    "        print(f\"\\n{group_label.upper()} – zu wenige Kategorien für Vergleiche.\")\n",
    "        return\n",
    "\n",
    "    p_raw, stats = pairwise_wilcoxon_categories(cat_df, min_pairs=min_pairs)\n",
    "    if not p_raw:\n",
    "        print(f\"\\n{group_label.upper()} – keine auswertbaren Kategorie-Paare (min_pairs={min_pairs}).\")\n",
    "        return\n",
    "\n",
    "    # Holm-Korrektur\n",
    "    p_adj = holm_correction(p_raw)\n",
    "\n",
    "    # Kategorie-Liste & p-Matrix (adj)\n",
    "    cats = list(cat_df.columns)\n",
    "    mat_adj = pd.DataFrame(1.0, index=cats, columns=cats, dtype=float)\n",
    "    for (c1, c2), p in p_adj.items():\n",
    "        mat_adj.loc[c1, c2] = mat_adj.loc[c2, c1] = p\n",
    "\n",
    "    # Signifikante Paare sammeln\n",
    "    rows = []\n",
    "    for (c1, c2), p in p_adj.items():\n",
    "        if p < alpha:\n",
    "            info = stats.get((c1, c2), {})\n",
    "            rows.append({\n",
    "                \"cat_1\": c1, \"cat_2\": c2,\n",
    "                \"p_adj\": p,\n",
    "                \"n_pairs\": info.get(\"n\", np.nan),\n",
    "                \"W\": info.get(\"W\", np.nan),\n",
    "                \"r_rb\": info.get(\"r_rb\", np.nan)\n",
    "            })\n",
    "    sig = pd.DataFrame(rows).sort_values(\"p_adj\") if rows else pd.DataFrame(columns=[\"cat_1\",\"cat_2\",\"p_adj\",\"n_pairs\",\"W\",\"r_rb\"])\n",
    "\n",
    "    # Ausgabe\n",
    "    print(f\"\\nWILCOXON – KATEGORIEVERGLEICHE ({group_label}) (agg={agg}, Holm, alpha={alpha}, min_pairs={min_pairs})\")\n",
    "    print(\"=\" * (len(group_label) + 61))\n",
    "    if sig.empty:\n",
    "        print(\"Keine signifikanten Unterschiede zwischen Kategorien.\")\n",
    "    else:\n",
    "        for _, r in sig.iterrows():\n",
    "            print(f\"- {r['cat_1']}  vs.  {r['cat_2']}  | p_adj = {r['p_adj']:.4f} | n={int(r['n_pairs'])} | W={r['W']:.1f} | r_rb={r['r_rb']:.3f}\")\n",
    "\n",
    "    print(\"\\nKomplette p-Wert-Matrix (Holm-korrigiert):\")\n",
    "    print(mat_adj.round(4))\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Aufrufe für beide Gruppen\n",
    "# -------------------------------\n",
    "run_wilcoxon_categories(\n",
    "    df=control_df,\n",
    "    question_texts=control_group_questions,\n",
    "    group_label=\"Control Group\",\n",
    "    comparable_questions=comparable_questions,\n",
    "    side_key=\"control\",\n",
    "    alpha=0.05,\n",
    "    min_pairs=5,   # Du kannst auf 3 senken, wenn Daten knapp sind\n",
    "    agg=\"mean\"     # oder \"median\"\n",
    ")\n",
    "\n",
    "run_wilcoxon_categories(\n",
    "    df=experimental_df,\n",
    "    question_texts=experimental_group_questions,\n",
    "    group_label=\"Experimental Group\",\n",
    "    comparable_questions=comparable_questions,\n",
    "    side_key=\"experimental\",\n",
    "    alpha=0.05,\n",
    "    min_pairs=5,\n",
    "    agg=\"mean\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
